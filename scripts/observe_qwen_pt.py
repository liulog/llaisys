# Note: This script is generated by ChatGPT partly.
# Please use carefully!

import argparse
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import os
import time
import sys
import io

sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8")

def torch_device(device_name: str, device_id=0):
    if device_name == "cpu":
        return torch.device("cpu")
    elif device_name == "nvidia":
        return torch.device(f"cuda:{device_id}")
    else:
        raise ValueError(f"Unsupported device name: {device_name}")

def load_hf_model(model_path=None, device_name="cpu"):
    model_id = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"

    if model_path and os.path.isdir(model_path):
        print(f"Loading model from local path: {model_path}")
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            device_map=torch_device(device_name),
            trust_remote_code=True,
        )
    else:
        print(f"Loading model from Hugging Face: {model_id}")
        tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.bfloat16,
            device_map=torch_device(device_name),
            trust_remote_code=True,
        )

    return tokenizer, model

def torch_rope(y: torch.Tensor, x: torch.Tensor, pos_ids: torch.Tensor, theta: float):
    assert y.dim() == 3
    seq_len, n_heads, head_dim = y.shape
    assert head_dim % 2 == 0, "Head dimension must be even for RoPE."

    # Split into [a, b] pairs
    x_a, x_b = x[..., : head_dim // 2], x[..., head_dim // 2 :]

    # [seq_len] positions starting from start_pos
    positions = pos_ids.to(torch.float32).unsqueeze(1)  # [seq_len, 1]

    # RoPE frequency exponents: 1 / theta^(2i / d)
    i = torch.arange(0, head_dim // 2, dtype=torch.float32, device=y.device)  # [1, head_dim//2]
    freqs = positions / (theta ** (2 * i / head_dim))  # [seq_len, head_dim//2]

    sin, cos = freqs.sin(), freqs.cos()
    sin = sin.unsqueeze(1)  # [seq_len, 1, dim/2]
    cos = cos.unsqueeze(1)

    # Apply rotation
    y[..., : head_dim // 2] = x_a * cos - x_b * sin
    y[..., head_dim // 2 :] = x_b * cos + x_a * sin

def torch_self_attention(attn_val, query, key, value, scale):
    query = query.transpose(-2, -3)
    key = key.transpose(-2, -3)
    value = value.transpose(-2, -3)
    L, S = query.size(-2), key.size(-2)
    attn_bias = torch.zeros(L, S, dtype=query.dtype, device=query.device)

    temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)
    attn_bias.masked_fill_(temp_mask.logical_not(), float("-inf"))
    attn_bias.to(query.dtype)

    key = key.repeat_interleave(query.size(-3) // key.size(-3), -3)
    value = value.repeat_interleave(query.size(-3) // value.size(-3), -3)

    attn_weight = query @ key.transpose(-2, -1) * scale
    attn_weight += attn_bias
    attn_weight = torch.softmax(attn_weight, dim=-1)
    attn_val.copy_((attn_weight @ value).transpose(-2, -3))

def hf_debug_model(prompt, tokenizer, model, max_tokens_preview=5):
    hook_outputs = {}

    def hook_tensor(name):
        def fn(module, input, output):
            hook_outputs[name] = output.detach().cpu()
        return fn

    # Register hook
    for i, block in enumerate(model.model.layers):
        block.input_layernorm.register_forward_hook(hook_tensor(f"layer_{i}_input_ln"))
        block.self_attn.q_proj.register_forward_hook(hook_tensor(f"layer_{i}_q_proj"))
        block.self_attn.k_proj.register_forward_hook(hook_tensor(f"layer_{i}_k_proj"))
        block.self_attn.v_proj.register_forward_hook(hook_tensor(f"layer_{i}_v_proj"))
        block.self_attn.o_proj.register_forward_hook(hook_tensor(f"layer_{i}_o_proj"))
        block.post_attention_layernorm.register_forward_hook(hook_tensor(f"layer_{i}_post_attn_ln"))
        block.mlp.register_forward_hook(hook_tensor(f"layer_{i}_mlp_out"))
        block.mlp.gate_proj.register_forward_hook(hook_tensor(f"layer_{i}_gate_proj"))
        block.mlp.up_proj.register_forward_hook(hook_tensor(f"layer_{i}_up_proj"))

    input_content = tokenizer.apply_chat_template(
        conversation=[{"role": "user", "content": prompt}],
        add_generation_prompt=True,
        tokenize=False,
    )
    torch.set_printoptions(linewidth=200)
    torch.set_printoptions(precision=6)
    torch.set_printoptions(threshold=1536)
    torch.set_printoptions(edgeitems=5)

    inputs = tokenizer.encode(input_content, return_tensors="pt").to(model.device)
    seqlen = inputs.shape[1]
    print(f"Input token IDs: {inputs[0].tolist()}")

    embedding_layer = model.get_input_embeddings()
    input_embeddings = embedding_layer(inputs)
    print(f"\nEmbedding shape: {input_embeddings.shape}")
    print(f"Embedding preview (first {max_tokens_preview} tokens):\n{input_embeddings[0,:max_tokens_preview,:]}")

    # forward
    with torch.no_grad():
        outputs = model(
            inputs,
            output_hidden_states=True,
            output_attentions=False,
            use_cache=False
        )

    hidden_states = outputs.hidden_states

    # logits
    lm_head = model.get_output_embeddings()  # nn.Linear(hidden_size, vocab_size)
    logits = outputs.logits          # [batch_size, seq_len, vocab_size]
    print(f"\nOutput logits shape: {logits.shape}")
    print(f"Output logits preview (first {max_tokens_preview} tokens):\n{logits[0,:max_tokens_preview,:]}")
    
    last_token_logits = logits[:, -1, :]     # [batch_size, vocab_size]
    predicted_token_id = last_token_logits.argmax(dim=-1)
    print(f"\nPredicted token id (last token): {predicted_token_id}")

    # 处理 Q/K/V + RoPE
    for name, tensor in hook_outputs.items():
        print(f"\nHook [{name}] shape: {tensor.shape}")
        print(f"Preview (first {max_tokens_preview} tokens):\n{tensor[0,:max_tokens_preview,:]}")
        if "_q_proj" in name or "_k_proj" in name in name:
            tensor_ = tensor[0] if tensor.dim() == 3 else tensor  # [seq_len, hidden_size]
            hidden_size = tensor_.shape[-1]
            if "_q_proj" in name:
                nhead = 12
                dim = 128
                nkvhead = 12
            else:  # k_proj / v_proj
                nhead = 12
                dim = 128
                nkvhead = 2
            group = nhead // nkvhead
            head_dim = dim

            if "_q_proj" in name:
                # Q reshape
                tensor_ = tensor_.view(seqlen, nhead, head_dim)
            else:
                # K/V reshape: [seq_len, nkvhead, head_dim] -> expand to nhead
                tensor_ = tensor_.view(seqlen, nkvhead, head_dim)
                tensor_ = tensor_.repeat(1, group, 1)  # [seq_len, nhead, head_dim]

            # 生成 position_ids
            position_ids = torch.arange(seqlen, device=tensor_.device)

            # RoPE
            torch_rope(tensor_, tensor_.clone(), position_ids, theta=10000.0)

            print(f"\n{name} shape after reshape & RoPE: {tensor_.shape}")
            print(f"Preview (first {max_tokens_preview} tokens):\n{tensor_[:max_tokens_preview]}")

    for i, h in enumerate(hidden_states):
        print(f"\nLayer {i} hidden states shape: {h.shape}")
        print(f"Preview (first {max_tokens_preview} tokens):\n{h[0,:max_tokens_preview,:]}")

    decoded = tokenizer.decode(inputs[0], skip_special_tokens=True)

    return {
        "decoded": decoded,
        "input_ids": inputs[0].tolist(),
        "embedding": input_embeddings[0],
        "hidden_states": hidden_states,
        "hook_outputs": hook_outputs
    }

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--device", default="cpu", choices=["cpu", "nvidia"], type=str)
    parser.add_argument("--model", default=None, type=str)
    parser.add_argument("--prompt", default="Who are you?", type=str)
    parser.add_argument("--max_steps", default=128, type=int)
    parser.add_argument("--top_p", default=0.8, type=float)
    parser.add_argument("--top_k", default=50, type=int)
    parser.add_argument("--temperature", default=1.0, type=float)
    parser.add_argument("--test", action="store_true")

    args = parser.parse_args()

    top_p, top_k, temperature = args.top_p, args.top_k, args.temperature
    if args.test:
        top_p, top_k, temperature = 1.0, 1, 1.0

    tokenizer, model = load_hf_model(args.model, args.device)
    print(model)
    
    start_time = time.time()
    result = hf_debug_model(prompt=args.prompt, tokenizer=tokenizer, model=model, max_tokens_preview=10)
    end_time = time.time()
    
    print("\nTime elapsed: {:.2f}s\n".format(end_time - start_time))

